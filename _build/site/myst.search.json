{"version":"1","records":[{"hierarchy":{"lvl1":"Certificates Of Freedom"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"Certificates Of Freedom"},"content":"This project uses Maryland State Archives’ “Legacy of Slavery Dataset Collection - Certificates of Freedom” as a case study for exploring the application of computational methods to archival collections.\n\nAuthor: Rajesh Kumar GNANASEKARAN\n\nReviewer: Richard MARCIANO\n\nCommunity Members: Maya DAVIS, Christopher HALEY (Maryland State Archives), Lyneise WILLIAMS (VERA Collaborative), Mark CONRAD (NARA)\n\nSource Available: \n\nhttps://​github​.com​/cases​-umd​/Legacy​-of​-Slavery\n\nLicense: \n\nCreative Commons - Attribute 4.0 Intl\n\nPrior Publications: 1. L. A. Perine, R. K. Gnanasekaran, P. Nicholas, A. Hill and R. Marciano, “Computational Treatments to Recover Erased Heritage: A Legacy of Slavery Case Study (CT-LoS),” 2020 IEEE International Conference on Big Data (Big Data), Atlanta, GA, USA, 2020, pp. 1894-1903, doi: 10.1109/BigData50022.2020.9378110. 2. P. Nicholas, R.K. Gnanasekaran, L. Perine, A. Hill, R. Marciano. (2020). Establishing a Research Agenda for Archival Science through Interdisciplinary Collaborations between Archivists and Technologists. 2020 SAA Research Forum (invited for submission and under consideration).","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl2":"Introduction"},"type":"lvl2","url":"/intro#introduction","position":2},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl2":"Introduction"},"content":"","type":"content","url":"/intro#introduction","position":3},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"The Legacy of Slavery Project at the Maryland State Archives (MSA)","lvl2":"Introduction"},"type":"lvl3","url":"/intro#the-legacy-of-slavery-project-at-the-maryland-state-archives-msa","position":4},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"The Legacy of Slavery Project at the Maryland State Archives (MSA)","lvl2":"Introduction"},"content":"This module is based on a case study involving \n\nThe “Legacy of Slavery Project” archival records from the Maryland State Archives. The Legacy of Slavery in Maryland is a major initiative of the Maryland State Archives. The program seeks to preserve and promote the vast universe of experiences that have shaped the lives of Maryland’s African American population. Over the last 18 years, some 420,000 individuals have been identified and data has been assembled into 16 major databases. The Maryland State Archives holds two essential types of records documenting freedom within its collections. The first are Manumissions, legal documents that frees an enslaved person from slavery on behalf of the slave holder. The second are Certificates of Freedom, documents that resulted from a 1805 General Assembly law that sought to identify Maryland’s free African American population and to control the availability of freedom papers. This legislation required those who were born free, or those who received their freedom from a slave owner, to record proof of their status in the county court. These documents, found in 111 record series at the Maryland State Archives arranged by the county of issuance, contain vital information about those who were enslaved. Everything from the names, age, physical description, location of the recipient’s birth and rearing, and names of the slave holder or witness who confirmed the person’s free status is usually included. These records have been uploaded and made available through the Legacy of Slavery searchable database. More information could be obtained from \n\nhere. The \n\nAIC has now partnered with the Maryland State Archives to help interpret this data and reveal hidden stories. This case study is performed on this dataset collection, referred to as the “Certificates of Freedom” (CoF) dataset.","type":"content","url":"/intro#the-legacy-of-slavery-project-at-the-maryland-state-archives-msa","position":5},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Certificates of Freedom in the State of Maryland","lvl2":"Introduction"},"type":"lvl3","url":"/intro#certificates-of-freedom-in-the-state-of-maryland","position":6},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Certificates of Freedom in the State of Maryland","lvl2":"Introduction"},"content":"A Certificate of Freedom is a legal document that was issued to African Americans who were required to record proof of their freedom in the county court. The court would then issue them a Certificate of Freedom. If the person had been previously manumitted by an act of the slaveholder, the court clerk or register of wills would look up the manumitting document before issuing a certificate of freedom.\n\nOne of the latest revelations which got public attention was the story of Smith Price, a founding member of the African Meeting House in Annapolis, which eventually became the First African Methodist Episcopal Church in 1803. According to Rebecca Morehouse \n\nhere, in 1980 excavations of certain burial remains were found which was later studied to be belonging to Mr. Smith Price, an enslaved later freed African American on whose land the Asbury United Methodist Church stands as of today. Several people, notably researcher Janice Hayes-Williams, worked hard to get approval for a reburial ceremony of Mr. Price’s remains at St Anne’s cemetery in Annapolis. A conjectural drawing of Smith Price by a forensic artist detective is shown here:\n\n\n\nSource\n\nUpon analysis in the Certificates of Freedom dataset collection, we were able to find Smith Price’s CoF as shown here:\n\nBy looking at the Smith Price’s CoF, the conjectural drawing and the article published above gives us a chance to connect to the lives of enslaved people and several such stories and insights are still buried deep in the documents as dataset collections at the MSA.\n\nOne other example is of an enslaved woman named “Lot Bell” who was identified in the CoF collection, and a forensic artist made a similar conjectural drawing of her as shown below: An article link \n\nhere describes the project below led by Chris Haley at the Maryland State Archives.\n\n\nThese images tell us that the CoF dataset collection is a repository of rich cultural and human values which when seen through the contemporary scholar’s lenses could unravel many more interesting and eyeopening insights as the ones above.\n\n","type":"content","url":"/intro#certificates-of-freedom-in-the-state-of-maryland","position":7},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Computational Thinking Framework","lvl2":"Introduction"},"type":"lvl3","url":"/intro#computational-thinking-framework","position":8},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Computational Thinking Framework","lvl2":"Introduction"},"content":"This Project is organized in steps based on the Computational Thinking framework presented by \n\nDavid Weintrop’s model of computation thinking\n\nOf these 4 major Practices, this Project applies 2 of them as indicated below:","type":"content","url":"/intro#computational-thinking-framework","position":9},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Computational Thinking Practices","lvl2":"Introduction"},"type":"lvl3","url":"/intro#computational-thinking-practices","position":10},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Computational Thinking Practices","lvl2":"Introduction"},"content":"","type":"content","url":"/intro#computational-thinking-practices","position":11},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl4":"Data Practices","lvl3":"Computational Thinking Practices","lvl2":"Introduction"},"type":"lvl4","url":"/intro#data-practices","position":12},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl4":"Data Practices","lvl3":"Computational Thinking Practices","lvl2":"Introduction"},"content":"Manipulating Data\n\nAnalyzing Data\n\nVisualizing Data","type":"content","url":"/intro#data-practices","position":13},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl4":"All Computational Problem Solving Practices","lvl3":"Computational Thinking Practices","lvl2":"Introduction"},"type":"lvl4","url":"/intro#all-computational-problem-solving-practices","position":14},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl4":"All Computational Problem Solving Practices","lvl3":"Computational Thinking Practices","lvl2":"Introduction"},"content":"","type":"content","url":"/intro#all-computational-problem-solving-practices","position":15},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl2":"Learning Goals"},"type":"lvl2","url":"/intro#learning-goals","position":16},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl2":"Learning Goals"},"content":"","type":"content","url":"/intro#learning-goals","position":17},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Archival Practices","lvl2":"Learning Goals"},"type":"lvl3","url":"/intro#archival-practices","position":18},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Archival Practices","lvl2":"Learning Goals"},"content":"Digital Records and Access Systems","type":"content","url":"/intro#archival-practices","position":19},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Ethics and Values Considerations","lvl2":"Learning Goals"},"type":"lvl3","url":"/intro#ethics-and-values-considerations","position":20},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl3":"Ethics and Values Considerations","lvl2":"Learning Goals"},"content":"Contexualized Analysis of Data\n\n","type":"content","url":"/intro#ethics-and-values-considerations","position":21},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl2":"Software and Tools"},"type":"lvl2","url":"/intro#software-and-tools","position":22},{"hierarchy":{"lvl1":"Certificates Of Freedom","lvl2":"Software and Tools"},"content":"Jupyter Notebooks and Jupyter Lab\n\nPython and libraries supported by it","type":"content","url":"/intro#software-and-tools","position":23},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1"},"type":"lvl1","url":"/part1","position":0},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1"},"content":"According to (Weintrop et al., 2015) “Data manipulation includes sorting, filtering, cleaning, normalizing, and joining disparate datasets. There are many strategies that can be employed when analyzing data for use in a scientific or mathematical context, including looking for patterns or anomalies, defining rules to categorize data, and identifying trends and correlations.” Below are the steps performed for data manipulation and analysis process using Python programming language on the Certificates of Freedom dataset. The modules are split into two parts - Part 1 and Part 2.\n\n","type":"content","url":"/part1","position":1},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl2":"Acquiring or Accessing the Data"},"type":"lvl2","url":"/part1#acquiring-or-accessing-the-data","position":2},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl2":"Acquiring or Accessing the Data"},"content":"The data for this project was originally crawled from the Maryland State Archives Legacy of Data collections. The data source is included in this module as a comma-separated values file. The link below will take you to a view the data file:\n\nLoS_CoF.csv\n\nThe dataset has 23,655 rows of data.\n\nTo process a csv file in Python, one of the first steps is to import a Python library called as ‘pandas’ which would help the program convert the csv file into a dataframe format or commonly called as a table format. We import the library into the program as below:\n\n# Importing libraries - pandas used for data science/data analysis and machine learning tasks and numpy - which provides support for multi-dimensional arrays\nimport pandas as pd\nimport numpy as np\n\nUsing the pandas library, we created a new dataframe in the name ‘df’ using read_csv function as shown below: After creating the dataframe, the print() function is used to display the top 10 rows loaded in the dataframe.\n\n# creating a data frame which is a table-like data structure that could read csv files, flat files, and other delimited data.\n# Converting input data into a data frame is a key starting point with Python programming language for big data analytics\n# Below command reads in the Certificates of Freedom dataset which should already be loaded in a folder called 'Datasets' as LoS_CoF.csv\ndf = pd.read_csv(\"https://raw.githubusercontent.com/cases-umd/Legacy-of-Slavery/refs/heads/master/Datasets/LoS_CoF.csv\") \n# Below command prints the first 10 records after the data is copied from the csv file\ndf.head(10)\n\nOf these features, the below ones were chosen to be cleaned and manipulated for use in the following steps for simplicity purposes.\n\nDate -- This indicates the date of issue of Certificate of Freedom\n\nPrior Status -- Prior status of the Enslaved person before issue of the CoF document\n\nHeight -- Height of the Enslaved person\n\nAge -- Age at the time of document issue\n\nWe anticipated errors and misinterpretation of names, numbers, etc. since this database was mostly transcribed manually by hand from the physical or scanned copies of the Certificates of Freedom. Our approach was to individually explore and clean the aforementioned columns utilizing the text and numerical operation functions in Python programming language for this purpose mostly. We looked at the dataset holistically at first, identifying features that allowed us to generate meaningful stories or visualizations. Upon confirmation of the features list, we analyzed each of them in detail to document bad data and eliminate them if possible, modify data types, exclude them from the final visualizations if found to be invalid, etc\n\nThis project involved team members from a diverse group of technology, historical, and archivist background. There were opportunities to work individually or to work in groups, but we decided to do a hybrid setup of analyzing alone and reporting the results back to the group for discussion. With respect to the analysis performed on the dataset, decisions were data-driven or historical facts driven.\n\nThrough researching the literature, conversations with historians and experts in the field, discussions with archivists from the Maryland State Archives, the team members followed a set of steps where certain unique characteristics of a particular feature for instance were identified and shared with the entire group for their inputs before finalizing the results\n\n","type":"content","url":"/part1#acquiring-or-accessing-the-data","position":3},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl2":"Date Feature (Issue of CoF)"},"type":"lvl2","url":"/part1#date-feature-issue-of-cof","position":4},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl2":"Date Feature (Issue of CoF)"},"content":"Through healthy discussions on what-if scenarios as most of the data were historical and we were bringing each of our expertise into the conversations, several insights were gleaned for specific columns which were vital to this Project. Also there were discussions on how data should be presented, collected, and analyzed without impacting the sensitivity of the people involved, especially since this set of collection was unique.\n\nOne of them is the date, there were different formats of date captured in the transcribed collection. This field is to indicate the date when the certificate of freedom was prepared and signed. There were a number of issues with this date field in the original dataset. Different date formats -- There were around 600 records with NULL value, a bunch of them with just YYYYMM format, most of them in the format YYYY-MM-DD and YYYYMMDD format.\n\n# Below command prints out the descriptive details of the column 'Date'\ndf[\"Date\"].describe()\n\n# Below command list the number of null or na values in the 'Date' column of the data frame\ndf[\"Date\"].isna().sum()\n\n# Below command displays an array of unique date values in the 'Date' column\ndf[\"Date\"].unique()\n\nAs could be seen above, there are different formats for the date column, some with missing month etc, some of these were manually verified for accuracy by checking the scanned documents from the MSA database as shown below:\n\n# Below command replaces all Null or nan values to the literal 'None' for ease of manipulation later in the process\ndf[\"Date\"]=df[\"Date\"].fillna('None')\ndf[\"Date\"].unique()\n\n","type":"content","url":"/part1#date-feature-issue-of-cof","position":5},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl3":"Below is a key function that identifies the bad date records and places a ‘NaT’ (Not a good date) value","lvl2":"Date Feature (Issue of CoF)"},"type":"lvl3","url":"/part1#below-is-a-key-function-that-identifies-the-bad-date-records-and-places-a-nat-not-a-good-date-value","position":6},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl3":"Below is a key function that identifies the bad date records and places a ‘NaT’ (Not a good date) value","lvl2":"Date Feature (Issue of CoF)"},"content":"\n\n# Below command creates a new column 'DateFormatted' on-the-fly (one of the cool things I like about python) and is copied with the results from the 'Date' column using a \n# transformation function called 'to_datetime()' by passing in the parameter 'error=coerce' which converts all erroneous date values into a string called 'NaT'\ndf['DateFormatted'] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n\n# Below command prints the unique converted date values from the newly created column and also displays 'NaT' for errorneous date values.\ndf[\"DateFormatted\"].unique()\n\n# Below command prints a sample of the output for the new columns 'Date' and 'DateFormatted' side-by-side to show how the original field values were transformed to a proper date\n# format and the bad values are given a 'NaT'\ndf[['Date','DateFormatted']]\n\nx = 0\nbad_date=[]\n# Below function is a loop function which processes each value of the new column 'DateFormatted' to check for invalid value marker 'NaT' and if found, it picks up the original \n# value from the 'Date' column and appends to a list. Once all the records are checked, it prints the unique values of this list using the 'set' function and the total number of \n# bad ones\nfor i in range(len(df['DateFormatted'])):\n    if pd.isna(df['DateFormatted'][i]):\n        bad_date.append(df['Date'][i])\n        x += 1\nprint(set(bad_date))\nprint(\"Number of Bad date records\", x)\nprint(\"Number of unique items in the Bad date\", len(set(bad_date)))\n\n# Below command displays the specific records that was identified as erroneously entered. The inner command 'df[]' first converts the 'Date' feature to a 'String' data type, and then uses another\n# in-built function to filter the records that match with the supplied criteria and the outer 'df[]' displays the results of that filtered records from the inner dataframe.\ndf[df['Date'].astype(str).str.strip()==\"184006\"]\n\nIn two of the instances, as seen below, the day of issue has not been found to be legible or visible, hence the MSA transcriber may have not been able recorded the date. There was no date but only month and year captured on the original CoF itself for c290 page 224 - Jeremiah Brown\n\n\n\n\nAnother instance of data entry error was for c290 page 185 Charles W Jones as shown below with the date captured as 1840516 instead of 18400516\n\nAll these 657 bad date records were identified and marked with ‘NaT’ and would not be used for further processing. However, the entire record was not removed as there could be other useful features that could provide us with some good insights. These ‘NaT’ date records would be shared with the MSA, to fix their Source of Record.\n\n","type":"content","url":"/part1#below-is-a-key-function-that-identifies-the-bad-date-records-and-places-a-nat-not-a-good-date-value","position":7},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl2":"Prior Status"},"type":"lvl2","url":"/part1#prior-status","position":8},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl2":"Prior Status"},"content":"This feature indicates the previous status of the enslaved person while the CoF document was being issued. This holds historical importance and needed carefule inputs from the historians. To address the issues with this feature in CoF dataset - Prior Status Column: Research was conducted to determine the prior status of those who were categorized as a “Descendant of a white female woman” as shown below from the set of unique categories. Source: Wikipedia - History of slavery in Maryland. This research was beneficial in identifying what group certain observations belong to.\n\n\n\n# df is the data frame variable which stores the entire dataset in a table form. Below command converts the specific column or feature 'PriorStatus' as Categorical type instead of String for manipulation\ndf[\"PriorStatus\"]=df[\"PriorStatus\"].astype('category')\n\nBelow list shows the different formats of the Prior Status as transcribed and it shows that these entries have to be grouped together by customized cleaning process.\n\n# After conversion, let's print the number of categories available for that particular feature from the dataset\nprint(set(df[\"PriorStatus\"]))\n\n","type":"content","url":"/part1#prior-status","position":9},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl3":"Below is a key IF function that tries to find for a good prior status value in each column and converts the value to a standard value. Others are given a ‘Unknown’","lvl2":"Prior Status"},"type":"lvl3","url":"/part1#below-is-a-key-if-function-that-tries-to-find-for-a-good-prior-status-value-in-each-column-and-converts-the-value-to-a-standard-value-others-are-given-a-unknown","position":10},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 1","lvl3":"Below is a key IF function that tries to find for a good prior status value in each column and converts the value to a standard value. Others are given a ‘Unknown’","lvl2":"Prior Status"},"content":"\n\n# As could be seen above, there are various types of Prior Status that are similar in nature. the value 'nan' in Python means it has no values.\n# Below set of commands form a component in Python called as a function. Functions are a block of commands which could be used to perform the same action every time they are called.\n# The below function converts the input parameter to the right Prior Status category based on some conditional statements.\ndef fix_prior_status(status):\n    # initiate variables to hold the literal value\n    free = \"free\"\n    born = \"born\"\n    enslaved = \"slave\"\n    descend = \"Descend\"\n    # conditional statements to use in-built 'find' function to check if the prior status passed has the value of the literal checked, and if so the status would be modified as mentioned\n    # in the 'return' statement\n    if status.find(born) != -1:\n        # it should also be noted that indentation is a key requirement with Python, not where the return statement starts after the 'if'\n        return \"Born Free\"\n    else:\n        # nested if's are possible in Python to conditionally control the else logic\n        if status.find(enslaved) != -1:\n            return \"Slave\"\n        else:\n            if status.find(descend) != -1:\n                return \"Born Free\"\n            else:\n                if status.find(free) != -1:\n                    return \"Free\"\n                else:\n                    return \"Unknown\"\n# Below command starts with the beginning indentation indicating a new set of commands outside of the function, even if its in the same cell block like shown here.\n# The 'apply' function applies the function definted above to the data frame's each records' Prior Status field avlue. \ndf[\"PriorStatusFormatted\"] = df[\"PriorStatus\"].apply(fix_prior_status)\n# The 'unique' in-built function prints out the distinct values of the transformed or modified prior status of the data frame\nprint(df[\"PriorStatusFormatted\"].unique())\n\nAs with the date field, those values that did not fall into one of these good categories they were assigned a ‘Unknown’ value.\n\n# Below commands help us to save the modified dataframe into a new output csv file which could be used in further steps of processing in the next notebook modules.\ndfo = pd.DataFrame(df)\ndfo.to_csv('Datasets/LoS_Clean_Output_Mod1.csv', index=False)","type":"content","url":"/part1#below-is-a-key-if-function-that-tries-to-find-for-a-good-prior-status-value-in-each-column-and-converts-the-value-to-a-standard-value-others-are-given-a-unknown","position":11},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2"},"type":"lvl1","url":"/part2","position":0},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2"},"content":"","type":"content","url":"/part2","position":1},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl2":"Height Feature"},"type":"lvl2","url":"/part2#height-feature","position":2},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl2":"Height Feature"},"content":"This field is to indicate the height of the individual freed in feet and inches.\n\n# import libraries used for data frame (table-like) operations, and numeric data structure operations\nimport pandas as pd\nimport numpy as np\n\n#code to import the csv saved from the previous step\ndf = pd.read_csv(\"https://raw.githubusercontent.com/cases-umd/Legacy-of-Slavery/refs/heads/master/Datasets/LoS_Clean_Output_Mod1.csv\") \n\n#code to pull the error above\ndf[\"Height\"]\n\n#code to pull the specific error above\ndf[\"Height\"].describe()\n\n","type":"content","url":"/part2#height-feature","position":3},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl4":"Below is a key function that coverts bad height format and feet + inches into a standardized inches format. Bad ones will be given a ‘NaN’","lvl2":"Height Feature"},"type":"lvl4","url":"/part2#below-is-a-key-function-that-coverts-bad-height-format-and-feet-inches-into-a-standardized-inches-format-bad-ones-will-be-given-a-nan","position":4},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl4":"Below is a key function that coverts bad height format and feet + inches into a standardized inches format. Bad ones will be given a ‘NaN’","lvl2":"Height Feature"},"content":"\n\n#code to split the Height into feet and inches\nimport re\n# this is a regex script which looks for a matching pattern in the input string. \n# https://docs.python.org/3/library/re.html\nr = re.compile(r\"([0-9]+)'([0-9]*\\.?[0-9]+|)\")\ndef format_height(el):\n    el_new =el.replace(\" \",\"\")\n    m = r.match(el_new)\n    if m == None:\n        return float('NaN')\n    else:\n        return int(m.group(1))*12 + (0 if not m.group(2) else float(m.group(2)))\n    \n# Some of the records have been transcribed as mixed fractions rather than decimal values. These values have to be converted to inches using different Python formula as discussed below.    \nfrom fractions import Fraction\ndef format_height_type2(el):  \n    el_new =el.replace('\"',\"\")\n    el_new =el_new.split('\\'')\n    el_new = [word for line in el_new for word in line.split()]\n    if not el_new:\n        return float('NaN')\n    else:\n        return int(el_new[0])*12 + (float(el_new[1]) + float(Fraction(el_new[2])))    \n# Below command starts with the beginning indentation indicating a new set of commands outside of the function, even if its in the same cell block like shown here.\n# The 'apply' function applies the function definted above to the data frame's each records' Prior Status field avlue. \ndf[\"Height_Inches\"] = df[\"Height\"].astype(str).apply(lambda x: format_height_type2(x) if x.find('/') != -1 else format_height(x))\n\n# show descriptive statistics\ndf[\"Height_Inches\"].describe()\n\n# show distinct height values\ndf[\"Height_Inches\"].unique()\n\nA study by (Margo & Steckel, 1982), which performed an analysis of the height vs age from the EnSlaved Mainfest data of around 50000+ enslaved people shipped between 1811 and 1861 to ports like Baltimore, Richmond and other cities from the Port of Savannah. According to this study, the average heights of tallest enslaved people was around 67 inches. In the same study where another set of Enslaved People’s appraisal records showed the maximum height was found to be around 72 inches. Found below are the images from this study showing the different heights by age.\n\nThe above charts raise doubts on the unique values we observed to be higher than 80 inches and lesser than 5 inches. Thus, separating these records from the dataframe below shows the different representation of the Height during transcription.\n\n# code to show bad records\ndf.loc[(df[\"Height_Inches\"]>80)|(df[\"Height_Inches\"]<5),['DataItem','Height','Height_Inches']]\n\nThe above values have to be manually handled by looking into the scanned documents and finding their right values as discussed below:\n\nOne of entries shown above where the height was mentioned as 4 feet 44.75 inches belonged to the enslaved person Milly Farmer c477-2, page 200, upon looking at the scanned document it was really captured as 4 feet 11.75 inches as found below from the document:\n\nAlso, it should be noted that there is record which was entered with a height of 9’.75\" which clearly seems like an impossible value. This had to be handled by attempting to manually look at the Certificate of Freedom record from the scanned documents. Upon analyzing we found that there was no CoF scanned document found for this person (Cof ID: 15696). It mentions that under note that this person was manumitted but we could not find the documents under Manumitted records as well. Hence, the height record was changed as NaN for this record.\n\n# Code to show bad records \ndf.loc[(df[\"Height_Inches\"].isna())&(df[\"Height\"].notna()),[\"DataItem\",\"Height\"]]\n\nOther data capture issues were corrected by looking at the original scanned CoF as shown below: the height was noted as 5 5” which was in fact 5” 5’ - 5 feet 5 inches\n\n\nFrom above code result, we identify that there are some invalid representations of the height where the transcribers did not follow the procedures to enter single quotes for Feet and double quotes for inches. These have to be manually handled as well.\n\n#code to manually update the issues identified above with the corrected value in inches\n# We directly use the dataitem id as shown above to update the records.\ndf[\"Height_Inches\"]\ndf.loc[(df[\"DataItem\"]==5034), \"Height_Inches\"] =59.75\ndf.loc[(df[\"DataItem\"]==5627), \"Height_Inches\"] =63.40\ndf.loc[(df[\"DataItem\"]==6199), \"Height_Inches\"] =None\ndf.loc[(df[\"DataItem\"]==15696), \"Height_Inches\"] =None\ndf.loc[(df[\"DataItem\"]==7861), \"Height_Inches\"] =65.00\ndf.loc[(df[\"DataItem\"]==11494), \"Height_Inches\"] =None\ndf.loc[(df[\"DataItem\"]==12839), \"Height_Inches\"] =60.00\ndf.loc[(df[\"DataItem\"]==15177), \"Height_Inches\"] =60.00\ndf.loc[(df[\"DataItem\"]==16966), \"Height_Inches\"] =61.00\n\n# show height values that are outliers and invalid values\ndf.loc[(df[\"Height_Inches\"]>80)|(df[\"Height_Inches\"]<5),['DataItem','Height','Height_Inches']]\ndf.loc[(df[\"Height_Inches\"].isna())&(df[\"Height\"].notna()),[\"DataItem\",\"Height\",\"Height_Inches\"]]\n\nAs could be seen, wherever possible, the team went through the scanned documents for the erroneous entries and updated the conversions ourselves. For these erroneous data, MSA would be contacted to fix the Source of Record.\n\n","type":"content","url":"/part2#below-is-a-key-function-that-coverts-bad-height-format-and-feet-inches-into-a-standardized-inches-format-bad-ones-will-be-given-a-nan","position":5},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl2":"Age feature"},"type":"lvl2","url":"/part2#age-feature","position":6},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl2":"Age feature"},"content":"Age field was originally in the text type format, converted to number, and converted all the decimals which was entered as it is from the original document listed as months into a 12 month per year relative decimal value, for example, the original CoF noted the enslaved person as 18 months old, the dataset had this value as 0.18 under the age column which actually should be 1.5 years old.\n\n","type":"content","url":"/part2#age-feature","position":7},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl3":"Below is a key function that converts bad decimal values transcrbed into a good decimal fraction of a year. For example, 0.18 was transcribed for 18 months, this was converted to 1.5 years.","lvl2":"Age feature"},"type":"lvl3","url":"/part2#below-is-a-key-function-that-converts-bad-decimal-values-transcrbed-into-a-good-decimal-fraction-of-a-year-for-example-0-18-was-transcribed-for-18-months-this-was-converted-to-1-5-years","position":8},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 2","lvl3":"Below is a key function that converts bad decimal values transcrbed into a good decimal fraction of a year. For example, 0.18 was transcribed for 18 months, this was converted to 1.5 years.","lvl2":"Age feature"},"content":"\n\n# code to pull the above \nimport math\nfrom fractions import Fraction\ndef format_fraction_age(x):  \n    fractional, whole = math.modf(x)\n    # Some of the records have been transcribed as bad fractions, this will fix it as a good fraction of a year\n    return float(fractional*100/12)   \n# The 'apply' function applies the function definted above to the data frame's each records' Age field value. \ndf[\"AgeFormatted\"] = df[\"Age\"].apply(lambda x: float(x) if x >= 1 else format_fraction_age(x))\n# import shlex\n\n# code to find the converted Age records\ndf.loc[(df[\"Age\"]<1),['DataItem','Age','AgeFormatted']]\n\nFor one case which was listed to be as 100 years old, upon checking the CoF original document, it’s unclear as the document shows something like eighty & twenty years as highlighted below: This is also noted in the notes section as “Age given as eighty and twenty years. Could potentially be 28 years, not 100.”\n\n# save the output file\n# save the output to the csv\ndfo = pd.DataFrame(df)\ndfo.to_csv('https://raw.githubusercontent.com/cases-umd/Legacy-of-Slavery/refs/heads/master/Datasets/LoS_Clean_Output_Mod2.csv', index=False)","type":"content","url":"/part2#below-is-a-key-function-that-converts-bad-decimal-values-transcrbed-into-a-good-decimal-fraction-of-a-year-for-example-0-18-was-transcribed-for-18-months-this-was-converted-to-1-5-years","position":9},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3"},"type":"lvl1","url":"/part3","position":0},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3"},"content":"# import important libraries used for visualization purposes\nimport pandas as pd\nimport networkx\nimport numpy as np\nimport geopandas as gpd\nimport shapely as shp\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n# import cufflinks\nimport plotly\n# word cloud library\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n# # Using plotly + cufflinks in offline mode\n# import cufflinks\n# cufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\n# matplotlib library\nimport matplotlib.pyplot as plt\n\n# import bokeh library which is a famous one for network analysis\nfrom bokeh.io import output_notebook, show, save\n\n# these are needed for Network Visualization below\nfrom bokeh.io import output_notebook, show, save\nfrom bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine\nfrom bokeh.plotting import figure\nfrom bokeh.plotting import from_networkx\n\n# these are needed for Geo Map visualization below\nimport plotly.figure_factory as ff\nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)    \n\n#reimport the csv saved from the previous step 2\n#code to import the csv saved from the previous step\ndf = pd.read_csv(\"https://raw.githubusercontent.com/cases-umd/Legacy-of-Slavery/refs/heads/master/Datasets/LoS_Clean_Output_Mod2.csv\") \ndf.head(10)\n\n# Below cufflinks package is to handle temp object which is a pandas.series dataframe 'df' created above which does not have a iplot method when not linked to plotly. \n# We need cufflinks to link plotly to pandas and add the iplot method:\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# Simple Histogram using iplot\ndf['AgeFormatted'].iplot(kind='hist', title='Histogram Chart of Enslaved Age vs Count')\n\n# Simple Histogram using iplot\ndf['Height_Inches'].iplot(kind='hist', title='Histogram Chart of Enslaved Height vs Count')\n\n# Below chart is a pie chart for Sex\nfig = px.pie(df, names='Sex',color_discrete_sequence=px.colors.sequential.RdBu,title=\"Pie Chart of Distribution of CoF over Sex\")\nfig.show()\n\n# Using the Plotly Express (Doc here -- https://plotly.com/python/plotly-express/), plotting a scatter plot showing Age on the y-axis with County on the x-axis.\nfig = px.scatter(df[df[\"County\"].notna()&df[\"AgeFormatted\"].notna()], x=\"County\", y=\"AgeFormatted\", color=\"County\", hover_name=\"Freed_FirstName\",\n                 title=\"County Vs Sex\")\n\nfig.show()\n\n# Creating new dataframe below with functions like datetime, grouper, agg, etc to create new fields from the original data for plotting charts as a grouped by\n# or to plot against the counts of the number of CoF's issued by Sex (for example as shown below) \ndfp=pd.DataFrame()\ndfp['CoFDate'] = pd.to_datetime(df['DateFormatted'])\ndfp['Sex']=df['Sex']\nfreq='Y'\n# The groupby function will aggregate the count of CoF's issued by Sex\ndfp = dfp.groupby(['Sex', pd.Grouper(key='CoFDate', freq=freq)])['Sex'].agg(['count']).reset_index()\nprint(dfp)\n# return a sorted DataFrame by date then count\ndfp = dfp.sort_values(by=['CoFDate', 'count'])\n# if you want to reset the index\ndfp = dfp.reset_index(drop=True)\n\n","type":"content","url":"/part3","position":1},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Interactive Line Chart Visualization of Sex of the Enslaved vs Year of the issue of CoF Document vs Counts"},"type":"lvl2","url":"/part3#interactive-line-chart-visualization-of-sex-of-the-enslaved-vs-year-of-the-issue-of-cof-document-vs-counts","position":2},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Interactive Line Chart Visualization of Sex of the Enslaved vs Year of the issue of CoF Document vs Counts"},"content":"\n\n# Plot the chart from the data created above\nfig = go.Figure()\nfig = px.area(dfp, x='CoFDate', y='count', color='Sex',title=\"CoF Issued Date vs Sex vs Counts\")\nfig.show()\n\n\n\nThe visualization above where there seems to be a spike in the issue of Certificates of Freedom around 1832 matches with historical events believed to have happened around the same period in MD state.\n\n","type":"content","url":"/part3#interactive-line-chart-visualization-of-sex-of-the-enslaved-vs-year-of-the-issue-of-cof-document-vs-counts","position":3},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Interactive Network Visualization and Analysis for the Enslaved and Owner (an example)"},"type":"lvl2","url":"/part3#interactive-network-visualization-and-analysis-for-the-enslaved-and-owner-an-example","position":4},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Interactive Network Visualization and Analysis for the Enslaved and Owner (an example)"},"content":"\n\n# this needs to be run for Bokeh library to be used\noutput_notebook()\n\n# The code below is a sample slice of the original dataset to isolate records belonging to a Slave Owner whose last name is 'Atwell'. This sliced data is then used to show the \n# network graph of the enslaved people owned by this single owner shown at the centre of the networking chart below:\nLoS_CoF_df = df.loc[(df[\"Owner_LastName\"]=='Atwell')]\nLoS_CoF = networkx.from_pandas_edgelist(LoS_CoF_df,'Owner_LastName','Freed_FirstName','DataItem')\n\n# Below are the steps required to plot a networking graph between the single owner with last name 'Atwell' from the CoF collection and the Enslaved people owned by this person.\nplt.figure(figsize=(8,8))\nnetworkx.draw(LoS_CoF, with_labels=True, node_color='skyblue', width=.3, font_size=8)\n#Choose a title!\ntitle = 'Legacy Of Slavery Certificates of Freedom - Enslaved Last Name vs Owner Last Name'\n\n#Establish which categories will appear when hovering over each node\nHOVER_TOOLTIPS = [(\"Freed_FirstName\", \"@index\")]\n\n#Create a plot — set dimensions, toolbar, and title\nplot = figure(tooltips = HOVER_TOOLTIPS,\n              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n\n#Create a network graph object with spring layout\n# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\nnetwork_graph = from_networkx(LoS_CoF, networkx.spring_layout, scale=10, center=(0, 0))\n\n#Set node size and color\nnetwork_graph.node_renderer.glyph = Circle(size=15, fill_color='skyblue')\n\n#Set edge opacity and width\nnetwork_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n\n#Add network graph to the plot\nplot.renderers.append(network_graph)\n\nshow(plot)\n#save(plot, filename=f\"{title}.html\")\n\n","type":"content","url":"/part3#interactive-network-visualization-and-analysis-for-the-enslaved-and-owner-an-example","position":5},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Interactive Geo Map Visualization of Maryland Counties from the Dataset (still work to be done)"},"type":"lvl2","url":"/part3#interactive-geo-map-visualization-of-maryland-counties-from-the-dataset-still-work-to-be-done","position":6},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Interactive Geo Map Visualization of Maryland Counties from the Dataset (still work to be done)"},"content":"\n\n# Below is an important transformation step where a new dataframe from the original data is created to map County Codes from the original CoF dataset to commonly used numeric code \n# called 'fips' code, which is used in geomapping services to automatically locate the area on the geo map. For example, a fips code of 24003 is assigned to CoF entries with County='AA' that \n# maps to 'Anne Arundel' county in MD based on MSA's classification here -- https://msa.maryland.gov/msa/speccol/sc2600/sc2685/html/abbrev.html\ndfg = pd.DataFrame()\ndfg= df\ndfg.loc[(df[\"County\"]=='AA'), \"County_Code\"] = \"24003\"\ndfg.loc[(df[\"County\"]=='AA'), \"County_New\"] =\"Anne Arundel\"\ndfg.loc[(df[\"County\"]=='AL'), \"County_Code\"] =\"24001\"\ndfg.loc[(df[\"County\"]=='AL'), \"County_New\"] =\"Allegany\"\ndfg.loc[(df[\"County\"]=='BA'), \"County_Code\"] =\"24510\"\ndfg.loc[(df[\"County\"]=='BA'), \"County_New\"] =\"Baltimore County\"\ndfg.loc[(df[\"County\"]=='BC'), \"County_Code\"] =\"24005\"\ndfg.loc[(df[\"County\"]=='BC'), \"County_New\"] =\"Baltimore City\"\ndfg.loc[(df[\"County\"]=='CA'), \"County_Code\"] =\"24011\"\ndfg.loc[(df[\"County\"]=='CA'), \"County_New\"] =\"Caroline\"\ndfg.loc[(df[\"County\"]=='CE'), \"County_Code\"] =\"24015\"\ndfg.loc[(df[\"County\"]=='CE'), \"County_New\"] =\"Cecil\"\ndfg.loc[(df[\"County\"]=='CH'), \"County_Code\"] =\"24017\"\ndfg.loc[(df[\"County\"]=='CH'), \"County_New\"] =\"Charles\"\ndfg.loc[(df[\"County\"]=='CR'), \"County_Code\"] =\"24013\"\ndfg.loc[(df[\"County\"]=='CR'), \"County_New\"] =\"Carroll\"\ndfg.loc[(df[\"County\"]=='CV'), \"County_Code\"] =\"24009\"\ndfg.loc[(df[\"County\"]=='CV'), \"County_New\"] =\"Calvert\"\ndfg.loc[(df[\"County\"]=='DO'), \"County_Code\"] =\"24019\"\ndfg.loc[(df[\"County\"]=='DO'), \"County_New\"] =\"Dorchester\"\ndfg.loc[(df[\"County\"]=='FR'), \"County_Code\"] =\"24021\"\ndfg.loc[(df[\"County\"]=='FR'), \"County_New\"] =\"Frederick\"\ndfg.loc[(df[\"County\"]=='GA'), \"County_Code\"] =\"24023\"\ndfg.loc[(df[\"County\"]=='GA'), \"County_New\"] =\"Garrett\"\ndfg.loc[(df[\"County\"]=='HA'), \"County_Code\"] =\"24025\"\ndfg.loc[(df[\"County\"]=='HA'), \"County_New\"] =\"Harford\"\ndfg.loc[(df[\"County\"]=='HO'), \"County_Code\"] =\"24027\"\ndfg.loc[(df[\"County\"]=='HO'), \"County_New\"] =\"Howard\"\ndfg.loc[(df[\"County\"]=='KE'), \"County_Code\"] =\"24029\"\ndfg.loc[(df[\"County\"]=='KE'), \"County_New\"] =\"Kent\"\ndfg.loc[(df[\"County\"]=='MO'), \"County_Code\"] =\"24031\"\ndfg.loc[(df[\"County\"]=='MO'), \"County_New\"] =\"Montgomery\"\ndfg.loc[(df[\"County\"]=='PG'), \"County_Code\"] =\"24033\"\ndfg.loc[(df[\"County\"]=='PG'), \"County_New\"] =\"Prince George's\"\ndfg.loc[(df[\"County\"]=='Qa'), \"County_Code\"] =\"24035\"\ndfg.loc[(df[\"County\"]=='Qa'), \"County_New\"] =\"Queen Anne's\"\ndfg.loc[(df[\"County\"]=='QA'), \"County_Code\"] =\"24035\"\ndfg.loc[(df[\"County\"]=='QA'), \"County_New\"] =\"Queen Anne's\"\ndfg.loc[(df[\"County\"]=='SM'), \"County_Code\"] =\"24037\"\ndfg.loc[(df[\"County\"]=='SM'), \"County_New\"] =\"St. Mary's\"\ndfg.loc[(df[\"County\"]=='SO'), \"County_Code\"] =\"24039\"\ndfg.loc[(df[\"County\"]=='SO'), \"County_New\"] =\"Somerset\"\ndfg.loc[(df[\"County\"]=='TA'), \"County_Code\"] =\"24041\"\ndfg.loc[(df[\"County\"]=='TA'), \"County_New\"] =\"Talbot\"\ndfg.loc[(df[\"County\"]=='WA'), \"County_Code\"] =\"24043\"\ndfg.loc[(df[\"County\"]=='WA'), \"County_New\"] =\"Washington\"\ndfg.loc[(df[\"County\"]=='WI'), \"County_Code\"] =\"24045\"\ndfg.loc[(df[\"County\"]=='WI'), \"County_New\"] =\"Wicomico\"\ndfg.loc[(df[\"County\"]=='WO'), \"County_Code\"] =\"24047\"\ndfg.loc[(df[\"County\"]=='WO'), \"County_New\"] =\"Worcester\"\n\n# A similar groupby and aggregate function is run to consolidate the counts of CoF's issued across each county for use in Geo map visualization below\nvalues=pd.DataFrame()\nvalues=dfg.groupby('County_Code').agg('count')\nvalues['fips']=values.index\nvalues1=dfg.groupby('County_New').agg('count')\nvalues['county_name']=values1.index\n\n# Plotly Mapbox tool is used to create a geo map with the fips and county counts of CoF's from previous steps with different color ranges, and an interactive map showing\n# MD state and the counties. The map automatically zooms to the MD state which is done using the zoom parameter and by adjusting the latitude and longitude values below. \nfig = px.choropleth_mapbox(values, geojson=counties, locations='fips', color='County',\n                           title='# of CoFs issued in MD State by Counties',\n                           color_continuous_scale=\"Edge\",\n                           range_color=(10, 5000),\n                           mapbox_style=\"carto-positron\",\n                           hover_name='county_name',\n                           zoom=6, center = {\"lat\": 39.0458, \"lon\": -76.641273},\n                           opacity=0.5,\n                           hover_data=['county_name'],\n                           labels={'County':'# Counts of CoF','county_name':'County Name'}\n                          )\nfig.update_layout(title='# of CoFs issued in MD State by Counties')\nfig.show()\n\n# This is another geo map visualization implemented using another Python plotting package called as Figure Factory. This is not as interactive as the Plotly Mapbox.\nimport shapely\nimport shapefile\nimport plotly\nfrom plotly.figure_factory._county_choropleth import create_choropleth\nimport plotly.figure_factory as ff\nfig = ff.create_choropleth(fips=values.index.to_list(), \n                           scope=['Maryland'],\n                           values=values.County.to_list(), \n                           title='MD State with Counties', \n                           round_legend_values=True,\n                           show_state_data=True,\n                           county_outline={'color': 'rgb(255,255,255)', 'width': 0.5},\n                           exponent_format=True,\n                           legend_title='# Counts of CoF')\nfig.layout.template = None\nfig.show()\n\n","type":"content","url":"/part3#interactive-geo-map-visualization-of-maryland-counties-from-the-dataset-still-work-to-be-done","position":7},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Word Cloud Visualization of an important feature (Notes) which are comments/remarks entered by transcribers."},"type":"lvl2","url":"/part3#word-cloud-visualization-of-an-important-feature-notes-which-are-comments-remarks-entered-by-transcribers","position":8},{"hierarchy":{"lvl1":"Certificates Of Freedom — Data Analysis Part 3","lvl2":"Word Cloud Visualization of an important feature (Notes) which are comments/remarks entered by transcribers."},"content":"\n\n# Start with the Notes feature:\ntext = df['Notes']\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text.to_string())\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n# Start with the Notes feature:\ntext = df['Notes']\n\nstopwords = set(STOPWORDS)\nstopwords.update([\"Anne\", \"Arundel\", \"Baltimore\", \"Arundel County\", \"Dorchester\",\"County\"])\n# Create and generate a word cloud image:\nwordcloud = WordCloud(stopwords=stopwords).generate(text.to_string())\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","type":"content","url":"/part3#word-cloud-visualization-of-an-important-feature-notes-which-are-comments-remarks-entered-by-transcribers","position":9},{"hierarchy":{"lvl1":"Computational Archival Science (CAS) Book"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Computational Archival Science (CAS) Book"},"content":"Computational Archival Science \n\nBook\n\nThis book provides resources and code notebooks for computational working with archival data.","type":"content","url":"/","position":1}]}